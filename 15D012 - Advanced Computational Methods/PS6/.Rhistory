t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
init <- 1/N
w       <<- rep( 1 / N , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula , data     = data , weights  = w , maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
init <<- 1/N
w       <<- rep( 1 / N , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula , data     = data , weights  = w , maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
### Clear workspace
rm(list = ls())
### Set working directory
setwd('/Users/felix/Documents/GSE/Term 2/15D012 Advanced Computational Methods/Probelmsets/GSE/15D012 - Advanced Computational Methods/PS6')
### Load Packages
if (!require("rpart")) 	 	install.packages("rpart");   	library(rpart)
### Initialize auxilliary functions
spam <- read.table('spambase.data', sep = ',')
################################################################################
# Section 1: Initialize k-Nearest-Neighbhor function
################################################################################
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
w       <- rep( init , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula , data     = data , weights  = w , maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
print(N)
w       <- rep( init , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula , data     = data , weights  = w , maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
## Clear workspace
rm(list = ls())
### Set working directory
setwd('/Users/felix/Documents/GSE/Term 2/15D012 Advanced Computational Methods/Probelmsets/GSE/15D012 - Advanced Computational Methods/PS6')
### Load Packages
if (!require("rpart")) 	 	install.packages("rpart");   	library(rpart)
### Initialize auxilliary functions
spam <- read.table('spambase.data', sep = ',')
################################################################################
# Section 1: Initialize k-Nearest-Neighbhor function
################################################################################
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
w       <- rep( 1/N , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula , data     = data , weights  = w , maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
### Clear workspace
rm(list = ls())
### Set working directory
setwd('/Users/felix/Documents/GSE/Term 2/15D012 Advanced Computational Methods/Probelmsets/GSE/15D012 - Advanced Computational Methods/PS6')
### Load Packages
if (!require("rpart")) 	 	install.packages("rpart");   	library(rpart)
### Initialize auxilliary functions
spam <- read.table('spambase.data', sep = ',')
################################################################################
# Section 1: Initialize k-Nearest-Neighbhor function
################################################################################
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
w       <<- rep( 1/N , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula , data     = data , weights  = w , maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
rm(w)
spam$V58 <- as.factor( ifelse( spam$V58 == 0, -1, 1 ) )
permutation <- sample.int(nrow(spam))
size <- floor( 0.8 * length(permutation) )
trl <- permutation[1:size]
tel <- setdiff(permutation, trl)
training.data <- spam[trl, ]
test.data     <- spam[tel,]
x.test <- test.data[,-1]
y.test <- test.data[,58]
model <- formula( V58 ~. )
nrow(training.data)
t01 <- adaBoost(formula = model ,
data = training.data ,
depth = 1,
noTrees = 2 )
t01 <- adaBoost(formula = model ,
data = training.data ,
depth = 1,
noTrees = 40 )
### Clear workspace
rm(list = ls())
### Set working directory
setwd('/Users/felix/Documents/GSE/Term 2/15D012 Advanced Computational Methods/Probelmsets/GSE/15D012 - Advanced Computational Methods/PS6')
### Load Packages
if (!require("rpart")) 	 	install.packages("rpart");   	library(rpart)
### Initialize auxilliary functions
spam <- read.table('spambase.data', sep = ',')
################################################################################
# Section 1: Initialize k-Nearest-Neighbhor function
################################################################################
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
w       <<- rep( 1/N , N )
print(w)
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula , data     = data , weights  = w , maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
spam$V58 <- as.factor( ifelse( spam$V58 == 0, -1, 1 ) )
permutation <- sample.int(nrow(spam))
size <- floor( 0.8 * length(permutation) )
trl <- permutation[1:size]
tel <- setdiff(permutation, trl)
training.data <- spam[trl, ]
test.data     <- spam[tel,]
x.test <- test.data[,-1]
y.test <- test.data[,58]
model <- formula( V58 ~. )
nrow(training.data)
t01 <- adaBoost(formula = model ,
data = training.data ,
depth = 1,
noTrees = 40 )
# Clear workspace
rm(list = ls())
### Set working directory
setwd('/Users/felix/Documents/GSE/Term 2/15D012 Advanced Computational Methods/Probelmsets/GSE/15D012 - Advanced Computational Methods/PS6')
### Load Packages
if (!require("rpart")) 	 	install.packages("rpart");   	library(rpart)
### Initialize auxilliary functions
spam <- read.table('spambase.data', sep = ',')
################################################################################
# Section 1: Initialize k-Nearest-Neighbhor function
################################################################################
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
w       <- rep( 1/N , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula ,
data     = data ,
weights  = w ,
maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
w       <- rep( (1/N) , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
if( is.null( test ) == FALSE ){
M <- nrow( test )
classifiers.test <- matrix( NA, nrow = M, ncol = noTrees )
}
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula ,
data     = data ,
weights  = w ,
maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
adaBoost <- function( formula , data , test = NULL, depth = 1, noTrees = 1 ) {
y       <- data[ , all.vars( formula )[1] ]
N       <- nrow( data )
w       <- rep( (1/N) , N )
global.alpha <- rep( NA, noTrees )
classifiers  <- matrix( NA, nrow = N, ncol = noTrees )
for( i in 1:noTrees ){
print(i)
temp.model <- rpart( formula  = formula ,
data     = data ,
weights  = w ,
maxdepth = depth )
print( i )
prediction <- predict( temp.model, data, type = 'class')
error.ind  <- ifelse( prediction == y , 1, 0 )
errors     <- sum( w * error.ind ) / sum( w )
alpha      <- log( ( 1 - errors ) / errors )
w          <- w * exp( alpha * error.ind )
global.alpha[i] <- alpha
classifiers[,i] <- prediction
if( is.null( test ) == FALSE ){
classifiers.test[,i] <- predict( temp.model, test, type = 'class' )
}
}
if( is.null( test ) == FALSE ){
final.prediction.training <- sign( classifiers      %*% global.alpha )
final.prediction.test     <- sign( classifiers.test %*% global.alpha )
output <- list( predLabels     = as.numeric( final.prediction.training ),
predLabelsTest = as.numeric( final.prediction.test ) )
} else {
final.prediction.training <- sign( classifiers      %*% global.alpha )
output <- list( predLabels = as.numeric( final.prediction.training ) )
}
return( output )
}
################################################################################
################################################################################
data     <- mtcars
data$mpg <- as.factor( ifelse( mtcars$mpg > 15, 1, -1 ) )
model    <- formula( mpg ~ . )
t01 <- adaBoost(formula = model, data = data , depth = 1, noTrees = 2 )
